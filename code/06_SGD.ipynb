{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Airline Passenger Satisfaction - Model 5: Stochastic Gradient Descent\n",
    "----\n",
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%run ./01_data_prep.ipynb\n",
    "%run utils.ipynb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using SGD with random hyperparameters and hinge loss function\n",
    "# Using a preprocessing pipeline to standardize the data\n",
    "scaler = StandardScaler()\n",
    "est = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "est.fit(X_train, y_train)\n",
    "est.predict(X_train)\n",
    "est.score(X_valid, y_valid)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# It is a good idea to scale the data when using SGD\n",
    "def scale_data(train, valid):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scaling data\n",
    "    X_train2 = scaler.transform(train)\n",
    "    X_valid2 = scaler.transform(valid)  # apply same transformation to test data\n",
    "    return (X_train2, X_valid2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# New classifier to try different hyperparams \n",
    "clf = SGDClassifier()\n",
    "\n",
    "X_train2, X_valid2 = scale_data(X_train, X_valid)\n",
    "\n",
    "# Hypertuning parameters\n",
    "# A good first guess for max_iter is ceil(10**6 / n) n = size of training data, doing 10k step iterations\n",
    "# Trying different loss functions\n",
    "# Trying an exponentially decreasing alpha\n",
    "param_grid = {'max_iter' : np.arange(10000, 100000, 10000), \"loss\" : [\"hinge\", \"log\"], \"alpha\" : 10.0**-np.arange(1,7)}\n",
    "\n",
    "# Cross validating data with 5 folds\n",
    "clf_gs = GridSearchCV(clf, param_grid, cv=5, n_jobs=10)\n",
    "\n",
    "clf_gs.fit(X_train2, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check score of best hyperparams\n",
    "clf_gs.best_score_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check best value of hyperparams\n",
    "clf_gs.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Constructing a complexity curve with different values for regularization parameter (alpha)\n",
    "\n",
    "p = list(10.0**-np.arange(1,5))\n",
    "lst_test =[]\n",
    "lst_train =[]\n",
    "for i in p:\n",
    "    clf2 = SGDClassifier(alpha = i, max_iter=60000, loss=\"hinge\")\n",
    "    X_train2, X_valid2 = scale_data(X_train, X_valid)\n",
    "    clf2.fit(X_train2, y_train)\n",
    "    z = clf2.score(X_valid2, y_valid)\n",
    "    t = clf2.score(X_train2, y_train)\n",
    "    lst_test.append(z)\n",
    "    lst_train.append(t)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(p, lst_test, color ='red', label='Test Accuracy')\n",
    "plt.plot(p, lst_train, color ='b', label='Train Accuracy', scalex=False)\n",
    "plt.xlabel('Alpha values --->')\n",
    "plt.title('Best value of alpha')\n",
    "plt.legend()\n",
    "plt.savefig('SGD_complexity.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model performed better on test data than training data with minor tuning of the alpha parameter (regularization), achieving similar results to tuning max_iter, alpha and loss function using GridSearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using alpha = 0.01\n",
    "model = SGDClassifier(alpha=0.01)\n",
    "\n",
    "plot_learning_curve(model, \"SGD Learning Curve\", X_train, y_train, n_jobs=2)\n",
    "plt.savefig(\"SGD Learning Curve.png\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "98f5bd2dc4419a3a2a6dfd5f48c070239cdc296a2553dbd89ed53d7c3a40ba3a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}