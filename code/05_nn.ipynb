{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Airline Passenger Satisfaction - Model 4: Neural Network\n",
    "----\n",
    "### Prepare data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "%run ./01_data_prep.ipynb\n",
    "%run ./utils.ipynb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run Classifier: NN\n",
    "#### Try to select best hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compare MAE with differing values of hyperparameters alpha and max_iter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# The scorers can be either one of the predefined metric strings or a scorer\n",
    "# callable, like the one returned by make_scorer\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score.\n",
    "# That estimator is made available at ``gs.best_estimator_`` along with\n",
    "# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n",
    "# ``gs.best_index_``\n",
    "gs = GridSearchCV(\n",
    "    MLPClassifier(random_state=42),\n",
    "    param_grid={\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "        'learning_rate_init': [0.0001, 0.001, 0.01, 0.05, 0.1]\n",
    "    },\n",
    "    scoring=scoring, refit='AUC', return_train_score=True\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "results = gs.cv_results_\n",
    "\n",
    "# 11683.2s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "# plt.grid()\n",
    "plt.title(\"GridSearchCV evaluating different alpha and learning_rate for MLP\")\n",
    "\n",
    "plt.xlabel(\"alpha/learning_rate\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "# ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0.2, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis_alpha = np.array(results['param_alpha'].data, dtype=float)\n",
    "X_axis_lr = np.array(results['param_learning_rate_init'].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), [True, False]):\n",
    "    train_style = 'o'\n",
    "    test_style = '.'\n",
    "    # --------------\n",
    "    # Alpha\n",
    "    # --------------\n",
    "    for sample, style in (('train', train_style), ('test', test_style)):\n",
    "        sample_score_mean = results[f'mean_{sample}_{scorer}']\n",
    "        sample_score_std = results[f'std_{sample}_{scorer}']\n",
    "        alpha_color = 'g' if color else 'k'\n",
    "        ax.plot(\n",
    "            X_axis_alpha,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=alpha_color,\n",
    "            alpha=1 if sample == 'test' else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index_alpha = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score_alpha = results['mean_test_%s' % scorer][best_index_alpha]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot(\n",
    "        [X_axis_alpha[best_index_alpha], ] * 2,\n",
    "        [0, best_score_alpha],\n",
    "        linestyle='-.',\n",
    "        color=alpha_color,\n",
    "        marker='x',\n",
    "        markeredgewidth=3,\n",
    "        ms=8\n",
    "    )\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\n",
    "        \"alpha %0.3f: %0.3f\" % (X_axis_alpha[best_index_alpha], best_score_alpha),\n",
    "        (X_axis_alpha[best_index_alpha], best_score_alpha + 0.005)\n",
    "    )\n",
    "\n",
    "    # --------------\n",
    "    # Learning Rate\n",
    "    # --------------\n",
    "    for sample, style in (('train', train_style), ('test', test_style)):\n",
    "        sample_score_mean = results[f'mean_{sample}_{scorer}']\n",
    "        sample_score_std = results[f'std_{sample}_{scorer}']\n",
    "        lr_color = 'r' if color else 'b'\n",
    "        # ax.fill_between(\n",
    "        #     X_axis_lr,\n",
    "        #     sample_score_mean - sample_score_std,\n",
    "        #     sample_score_mean + sample_score_std,\n",
    "        #     alpha=0.1 if sample == 'test' else 0,\n",
    "        #     color=lr_color\n",
    "        # )\n",
    "        ax.plot(\n",
    "            X_axis_lr,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=lr_color,\n",
    "            alpha=1 if sample == 'test' else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index_lr = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score_lr = results['mean_test_%s' % scorer][best_index_lr]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot(\n",
    "        [X_axis_lr[best_index_lr], ] * 2,\n",
    "        [0, best_score_lr],\n",
    "        linestyle='-.',\n",
    "        color=lr_color,\n",
    "        marker='x',\n",
    "        markeredgewidth=3,\n",
    "        ms=8\n",
    "    )\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\n",
    "        \"lr %0.3f: score %0.3f\" % (X_axis_lr[best_index_lr], best_score_lr),\n",
    "        (X_axis_lr[best_index_lr], best_score_lr + 0.005)\n",
    "    )\n",
    "\n",
    "print(gs.best_params_)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}